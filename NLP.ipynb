{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "011e98a4-84bf-4e2d-8261-e9827602c47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\win11\\anaconda3\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\win11\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac2f55fb-9e14-46bd-8cc3-d60f7420ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283b2ec-5e85-4364-812c-cd68be596511",
   "metadata": {},
   "source": [
    "#Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c1d83d0-1b36-4353-98fe-bfce279c705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fb489df-ddbc-4ff0-a175-f2721a326613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I like to play cricket, but what I do in that sport is different—I've been here for over 12 years, and I've heard what people were saying: 'Well, your father should have been playing cricket in this country by now'. The first thing to know is that people\"},\n",
       " {'generated_text': 'Hello, I like to play cricket, sometimes, but they\\'re the same rules,\" said his nephew, who has been playing cricket for many years and has not been seen on television.\\n\\n\\'One bad cricket match might not be enough\\'\\n\\nA year ago the retired ex-colleg'},\n",
       " {'generated_text': \"Hello, I like to play cricket, but I've never been in a place that can get me started as a bat pro, so that has to be my goal.\\n\\nWith the Ashes there is just too much talk about the World Cup qualifiers and how they don't get any more important.\"},\n",
       " {'generated_text': 'Hello, I like to play cricket, not cricket.\"\\n\\nAnd you\\'re not the smartest.\\n\\n\"Yeah, yeah, yeah, yeah. Actually, I\\'m pretty good at cricket. I love it. I got the grades in Math in my college, but never really got up to'},\n",
       " {'generated_text': 'Hello, I like to play cricket, I like to play cricket in England, I love India. But I don\\'t understand why cricket is a sport I can\\'t play to\".\\n\\nSohil, whose house in Rajasthan has been given the state seal, has no other choice but'},\n",
       " {'generated_text': 'Hello, I like to play cricket, I like to play football and I like to love cricket and I want to spend my time on it.\"\\n\\nHe played a day and a half after his first game, and the only thing that remained was playing his first international game. Asked if it helped'},\n",
       " {'generated_text': 'Hello, I like to play cricket, I guess. Do you know how many batsmen have ever walked onto the field? Five, because they have. That\\'s what I did.\"\\n\\nA few years ago after finishing third in the 2011 Test rankings, he told reporters: \"The only way'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea7c9cbc-17a2-46af-94eb-9a72c95ad7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Natural Language Processing is evolving technology that is capable of'},\n",
       " {'generated_text': 'Natural Language Processing is evolving technology (through new models'},\n",
       " {'generated_text': 'Natural Language Processing is evolving technology, with new opportunities'},\n",
       " {'generated_text': 'Natural Language Processing is evolving technology to bring the entire'},\n",
       " {'generated_text': 'Natural Language Processing is evolving technology that will enable humans'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Natural Language Processing is evolving technology\", max_length=10, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af08748-dcf4-431d-b5ed-4a2bec2deb58",
   "metadata": {},
   "source": [
    "#Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06d060e5-f08f-4f5e-84ca-c380dc8d165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6062517762184143,\n",
       " 'start': 42,\n",
       " 'end': 97,\n",
       " 'answer': '\"For every action there is equal and opposite reaction\"'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "question_answerer({\n",
    "    'question': 'What is the Newtons third law of motion?',\n",
    "    'context': 'Newton’s third law of motion states that, \"For every action there is equal and opposite reaction\"'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c095b84f-605b-4dd4-833b-35986631f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1: 'trial division'\n",
      "Answer 2: '1975'\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Micorsoft was founded by Bill gates and Paul allen in the year 1975.\n",
    "The property of being prime (or not) is called primality.\n",
    "A simple but slow method of verifying the primality of a given number n is known as trial division.\n",
    "It consists of testing whether n is a multiple of any integer between 2 and itself.\n",
    "Algorithms much more efficient than trial division have been devised to test the primality of large numbers.\n",
    "These include the Miller–Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\n",
    "Particularly fast methods are available for numbers of special forms, such as Mersenne numbers.\n",
    "As of January 2016, the largest known prime number has 22,338,618 decimal digits.\n",
    "\"\"\"\n",
    "\n",
    "result = nlp(question=\"What is a simple method to verify primality?\", context=context)\n",
    "\n",
    "print(f\"Answer 1: '{result['answer']}'\")\n",
    "\n",
    "result = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\n",
    "\n",
    "print(f\"Answer 2: '{result['answer']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73bbc9-23f1-4fd4-9637-b7d1778feb6a",
   "metadata": {},
   "source": [
    "#Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c0fa2e0-81eb-4715-a8c5-f5eb84cf7c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German:  Joe Biden wurde der 46. Präsident der USA.\n"
     ]
    }
   ],
   "source": [
    "# English to German\n",
    "translator_ger = pipeline(\"translation_en_to_de\")\n",
    "print(\"German: \",translator_ger(\"Joe Biden became the 46th president of U.S.A.\", max_length=40)[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d263a695-5ec3-435f-a5a4-9f5d7c0302c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French:  Joe Biden est devenu le 46e président des États-Unis\n"
     ]
    }
   ],
   "source": [
    "translator_fr = pipeline('translation_en_to_fr')\n",
    "print(\"French: \",translator_fr(\"Joe Biden became the 46th president of U.S.A\",  max_length=40)[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646e10e-faa9-4a79-8582-24108bf69c01",
   "metadata": {},
   "source": [
    "#Text-Summerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76245050-da1f-4a1f-938e-d4eed8497582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Win11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Win11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractive summarization involves selecting and combining important sentences or phrases from the original text to create a summary. Abstractive summarization, on the other hand, involves generating new sentences that capture the key information from the original text. There are two main approaches to text summarization: extractive and abstractive. Text summarization is one of the applications of NLP, where the goal is to generate a concise and coherent summary of a given document or piece of text. Let's try generating a summary for this sample text using our program.\n"
     ]
    }
   ],
   "source": [
    "#using nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def read_article(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stop_words):\n",
    "    words1 = nltk.word_tokenize(sent1)\n",
    "    words2 = nltk.word_tokenize(sent2)\n",
    "\n",
    "    words1 = [word.lower() for word in words1 if word.isalnum() and word.lower() not in stop_words]\n",
    "    words2 = [word.lower() for word in words2 if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    all_words = list(set(words1 + words2))\n",
    "\n",
    "    vector1 = [1 if word in words1 else 0 for word in all_words]\n",
    "    vector2 = [1 if word in words2 else 0 for word in all_words]\n",
    "\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def generate_summary(text, num_sentences=5):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    sentences = read_article(text)\n",
    "\n",
    "    similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    sentence_scores = np.array(similarity_matrix.sum(axis=1))\n",
    "\n",
    "    ranked_sentences = np.argsort(-sentence_scores)\n",
    "\n",
    "    summary_sentences = [sentences[i] for i in ranked_sentences[:num_sentences]]\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return summary\n",
    "\n",
    "input_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. It involves the development of algorithms and models that enable computers to understand, interpret, and generate human-like text. Text summarization is one of the applications of NLP, where the goal is to generate a concise and coherent summary of a given document or piece of text.\n",
    "\n",
    "There are two main approaches to text summarization: extractive and abstractive. Extractive summarization involves selecting and combining important sentences or phrases from the original text to create a summary. Abstractive summarization, on the other hand, involves generating new sentences that capture the key information from the original text.\n",
    "\n",
    "In this example, we will focus on extractive summarization using NLTK, a popular NLP library in Python. We will use a simple algorithm that calculates the cosine similarity between sentences to identify the most important ones for the summary.\n",
    "\n",
    "Let's try generating a summary for this sample text using our program.\n",
    "\"\"\"\n",
    "\n",
    "summary = generate_summary(input_text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2fb2b25-721e-4d6d-9c56-5da7f71bb42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\win11\\anaconda3\\lib\\site-packages (2.0.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.1.2 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.1.2 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.1.2 which is incompatible.\n",
      "numba 0.59.0 requires numpy<1.27,>=1.22, but you have numpy 2.1.2 which is incompatible.\n",
      "pandas 2.1.4 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.1.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.1.2 which is incompatible.\n",
      "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.1.2 which is incompatible.\n",
      "streamlit 1.30.0 requires numpy<2,>=1.19.3, but you have numpy 2.1.2 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.2 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 2.1.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Using cached numpy-2.1.2-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed numpy-2.1.2\n",
      "Found existing installation: blis 1.0.1\n",
      "Uninstalling blis-1.0.1:\n",
      "  Successfully uninstalled blis-1.0.1\n",
      "Found existing installation: spacy 3.8.2\n",
      "Uninstalling spacy-3.8.2:\n",
      "  Successfully uninstalled spacy-3.8.2\n",
      "Collecting blis\n",
      "  Using cached blis-1.0.1-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=2.0.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from blis) (2.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy<3.0.0,>=2.0.0 (from blis)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\win11\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\win11\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Using cached blis-1.0.1-cp311-cp311-win_amd64.whl (6.3 MB)\n",
      "Using cached spacy-3.8.2-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "Using cached numpy-2.0.2-cp311-cp311-win_amd64.whl (15.9 MB)\n",
      "Installing collected packages: numpy, blis, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed blis-1.0.1 numpy-2.0.2 spacy-3.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.59.0 requires numpy<1.27,>=1.22, but you have numpy 2.0.2 which is incompatible.\n",
      "pandas 2.1.4 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.0.2 which is incompatible.\n",
      "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.2 which is incompatible.\n",
      "streamlit 1.30.0 requires numpy<2,>=1.19.3, but you have numpy 2.0.2 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"C:\\Users\\Win11\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py\", line 25, in <module>\n",
      "    from . import _errors\n",
      "  File \"h5py\\_errors.pyx\", line 1, in init h5py._errors\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "BLIS support requires blis: pip install blis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install blis spacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m spacy download en_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstop_words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STOP_WORDS\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[0;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\api.py:23\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     configure_normal_init,\n\u001b[0;32m     18\u001b[0m     glorot_uniform_init,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     zero_init,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     LSTM,\n\u001b[0;32m     25\u001b[0m     CauchySimilarity,\n\u001b[0;32m     26\u001b[0m     ClippedLinear,\n\u001b[0;32m     27\u001b[0m     Dish,\n\u001b[0;32m     28\u001b[0m     Dropout,\n\u001b[0;32m     29\u001b[0m     Embed,\n\u001b[0;32m     30\u001b[0m     Gelu,\n\u001b[0;32m     31\u001b[0m     HardSigmoid,\n\u001b[0;32m     32\u001b[0m     HardSwish,\n\u001b[0;32m     33\u001b[0m     HardSwishMobilenet,\n\u001b[0;32m     34\u001b[0m     HardTanh,\n\u001b[0;32m     35\u001b[0m     HashEmbed,\n\u001b[0;32m     36\u001b[0m     LayerNorm,\n\u001b[0;32m     37\u001b[0m     Linear,\n\u001b[0;32m     38\u001b[0m     Logistic,\n\u001b[0;32m     39\u001b[0m     Maxout,\n\u001b[0;32m     40\u001b[0m     Mish,\n\u001b[0;32m     41\u001b[0m     MultiSoftmax,\n\u001b[0;32m     42\u001b[0m     MXNetWrapper,\n\u001b[0;32m     43\u001b[0m     ParametricAttention,\n\u001b[0;32m     44\u001b[0m     ParametricAttention_v2,\n\u001b[0;32m     45\u001b[0m     PyTorchLSTM,\n\u001b[0;32m     46\u001b[0m     PyTorchRNNWrapper,\n\u001b[0;32m     47\u001b[0m     PyTorchWrapper,\n\u001b[0;32m     48\u001b[0m     PyTorchWrapper_v2,\n\u001b[0;32m     49\u001b[0m     PyTorchWrapper_v3,\n\u001b[0;32m     50\u001b[0m     Relu,\n\u001b[0;32m     51\u001b[0m     ReluK,\n\u001b[0;32m     52\u001b[0m     Sigmoid,\n\u001b[0;32m     53\u001b[0m     Softmax,\n\u001b[0;32m     54\u001b[0m     Softmax_v2,\n\u001b[0;32m     55\u001b[0m     SparseLinear,\n\u001b[0;32m     56\u001b[0m     SparseLinear_v2,\n\u001b[0;32m     57\u001b[0m     Swish,\n\u001b[0;32m     58\u001b[0m     TensorFlowWrapper,\n\u001b[0;32m     59\u001b[0m     TorchScriptWrapper_v1,\n\u001b[0;32m     60\u001b[0m     add,\n\u001b[0;32m     61\u001b[0m     array_getitem,\n\u001b[0;32m     62\u001b[0m     bidirectional,\n\u001b[0;32m     63\u001b[0m     chain,\n\u001b[0;32m     64\u001b[0m     clone,\n\u001b[0;32m     65\u001b[0m     concatenate,\n\u001b[0;32m     66\u001b[0m     expand_window,\n\u001b[0;32m     67\u001b[0m     keras_subclass,\n\u001b[0;32m     68\u001b[0m     list2array,\n\u001b[0;32m     69\u001b[0m     list2padded,\n\u001b[0;32m     70\u001b[0m     list2ragged,\n\u001b[0;32m     71\u001b[0m     map_list,\n\u001b[0;32m     72\u001b[0m     noop,\n\u001b[0;32m     73\u001b[0m     padded2list,\n\u001b[0;32m     74\u001b[0m     premap_ids,\n\u001b[0;32m     75\u001b[0m     pytorch_to_torchscript_wrapper,\n\u001b[0;32m     76\u001b[0m     ragged2list,\n\u001b[0;32m     77\u001b[0m     reduce_first,\n\u001b[0;32m     78\u001b[0m     reduce_last,\n\u001b[0;32m     79\u001b[0m     reduce_max,\n\u001b[0;32m     80\u001b[0m     reduce_mean,\n\u001b[0;32m     81\u001b[0m     reduce_sum,\n\u001b[0;32m     82\u001b[0m     remap_ids,\n\u001b[0;32m     83\u001b[0m     remap_ids_v2,\n\u001b[0;32m     84\u001b[0m     residual,\n\u001b[0;32m     85\u001b[0m     resizable,\n\u001b[0;32m     86\u001b[0m     siamese,\n\u001b[0;32m     87\u001b[0m     sigmoid_activation,\n\u001b[0;32m     88\u001b[0m     softmax_activation,\n\u001b[0;32m     89\u001b[0m     strings2arrays,\n\u001b[0;32m     90\u001b[0m     tuplify,\n\u001b[0;32m     91\u001b[0m     uniqued,\n\u001b[0;32m     92\u001b[0m     with_array,\n\u001b[0;32m     93\u001b[0m     with_array2d,\n\u001b[0;32m     94\u001b[0m     with_cpu,\n\u001b[0;32m     95\u001b[0m     with_debug,\n\u001b[0;32m     96\u001b[0m     with_flatten,\n\u001b[0;32m     97\u001b[0m     with_flatten_v2,\n\u001b[0;32m     98\u001b[0m     with_getitem,\n\u001b[0;32m     99\u001b[0m     with_list,\n\u001b[0;32m    100\u001b[0m     with_nvtx_range,\n\u001b[0;32m    101\u001b[0m     with_padded,\n\u001b[0;32m    102\u001b[0m     with_ragged,\n\u001b[0;32m    103\u001b[0m     with_reshape,\n\u001b[0;32m    104\u001b[0m     with_signpost_interval,\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    107\u001b[0m     CategoricalCrossentropy,\n\u001b[0;32m    108\u001b[0m     CosineDistance,\n\u001b[0;32m    109\u001b[0m     L2Distance,\n\u001b[0;32m    110\u001b[0m     SequenceCategoricalCrossentropy,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    113\u001b[0m     Model,\n\u001b[0;32m    114\u001b[0m     change_attr_values,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     wrap_model_recursive,\n\u001b[0;32m    119\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\layers\\__init__.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclipped_linear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClippedLinear, HardSigmoid, HardTanh, ReluK\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcatenate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concatenate\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdish\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dish\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdropout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dropout\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\layers\\concatenate.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_width\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnoop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m noop\n\u001b[1;32m---> 21\u001b[0m NUMPY_OPS \u001b[38;5;241m=\u001b[39m NumpyOps()\n\u001b[0;32m     24\u001b[0m InT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mAny)\n\u001b[0;32m     25\u001b[0m OutT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mUnion[Array2d, Sequence[Array2d], Ragged])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\backends\\numpy_ops.pyx:63\u001b[0m, in \u001b[0;36mthinc.backends.numpy_ops.NumpyOps.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: BLIS support requires blis: pip install blis"
     ]
    }
   ],
   "source": [
    "#using spacy\n",
    "!pip install --upgrade numpy\n",
    "\n",
    "!pip uninstall blis spacy -y\n",
    "!pip install blis spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Generate summary function\n",
    "def generate_summary(text, num_sentences=5):\n",
    "    # Split the text into sentences\n",
    "    sentences = [sent.text for sent in nlp(text).sents]\n",
    "    \n",
    "    # Preprocess each sentence\n",
    "    preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Vectorize the sentences\n",
    "    vectorizer = CountVectorizer().fit_transform(preprocessed_sentences)\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(vectorizer, vectorizer)\n",
    "    \n",
    "    # Score each sentence based on its similarity to other sentences\n",
    "    sentence_scores = similarity_matrix.sum(axis=1)\n",
    "    \n",
    "    # Rank sentences based on their score\n",
    "    ranked_sentences = np.argsort(sentence_scores)[::-1]\n",
    "    \n",
    "    # Select top N sentences\n",
    "    summary_sentences = [sentences[i] for i in ranked_sentences[:num_sentences]]\n",
    "    \n",
    "    # Join the selected sentences to form the final summary\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary\n",
    "\n",
    "# Sample input text\n",
    "input_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. It involves the development of algorithms and models that enable computers to understand, interpret, and generate human-like text. Text summarization is one of the applications of NLP, where the goal is to generate a concise and coherent summary of a given document or piece of text.\n",
    "\n",
    "There are two main approaches to text summarization: extractive and abstractive. Extractive summarization involves selecting and combining important sentences or phrases from the original text to create a summary. Abstractive summarization, on the other hand, involves generating new sentences that capture the key information from the original text.\n",
    "\n",
    "In this example, we will focus on extractive summarization using spaCy, a popular NLP library in Python. We will use a simple algorithm that calculates the cosine similarity between sentences to identify the most important ones for the summary.\n",
    "\n",
    "Let's try generating a summary for this sample text using our program.\n",
    "\"\"\"\n",
    "\n",
    "# Generate and print the summary\n",
    "summary = generate_summary(input_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd174bd-0f53-476a-add2-01aab3fc5625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
